{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention Glove_Task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MwQbYAFwmMo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "27b1336f-5e1a-46f3-be5f-3901ca6f501d"
      },
      "source": [
        "%tensorflow_version 1.x   #Using tensorflow 1.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':      #use GPU for fast process\n",
        "  raise SystemError('GPU device not found')\n",
        "#print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.x   #Using tensorflow 1.x`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAV9fUzI6Sks",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3334295f-a713-456b-81b8-6f93cfad4d6c"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ6MP-TptKog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3423760-e89d-4970-8030-1f50b4dc13ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O05nwxAVWv1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests  \n",
        "file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"          #Download glove here is the link\n",
        "    \n",
        "r = requests.get(file_url, stream = True)  \n",
        "  \n",
        "with open(\"glove.6B.zip\", \"wb\") as file:  \n",
        "    for block in r.iter_content(chunk_size = 1024): \n",
        "         if block:  \n",
        "             file.write(block) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7as22nDWy05",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "49147d44-418c-49ff-819f-996417ae61b6"
      },
      "source": [
        "!apt install unzip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 0%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 6%\r\rReading package lists... 62%\r\rReading package lists... 62%\r\rReading package lists... 63%\r\rReading package lists... 63%\r\rReading package lists... 71%\r\rReading package lists... 71%\r\rReading package lists... 71%\r\rReading package lists... 71%\r\rReading package lists... 71%\r\rReading package lists... 80%\r\rReading package lists... 80%\r\rReading package lists... 80%\r\rReading package lists... 80%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 81%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 87%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 93%\r\rReading package lists... 94%\r\rReading package lists... 94%\r\rReading package lists... 95%\r\rReading package lists... 95%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... 98%\r\rReading package lists... Done\r\n",
            "\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 0%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree... 50%\r\rBuilding dependency tree       \r\n",
            "\rReading state information... 0%\r\rReading state information... 0%\r\rReading state information... Done\r\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhtZ4qcSXb8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "58b48c76-4e75-4c77-f53a-ee9eb74998a0"
      },
      "source": [
        "cd '/content/go'      #downloaded in server but not in my drive it is fast to access"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/go #downloaded in server but not in my drive it is fast to access'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycvra3e0XNpq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "44df1c7b-4408-423e-805d-a82d80455177"
      },
      "source": [
        "!unzip  '/content/glove.6B.zip' -d go"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/glove.6B.zip\n",
            "replace go/glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: go/glove.6B.50d.txt     \n",
            "replace go/glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: go/glove.6B.100d.txt    y\n",
            "\n",
            "replace go/glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: go/glove.6B.200d.txt    y\n",
            "\n",
            "replace go/glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename:   inflating: go/glove.6B.300d.txt    y\n",
            "y\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykMyHL-keYKz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd102c52-7838-4513-88e3-146b271c3dd0"
      },
      "source": [
        "'''import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D,GlobalAveragePooling1D\n",
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MceJKUmGgWQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILE = '/content/go/glove.6B.300d.txt'      \n",
        "train = pd.read_csv('/content/drive/My Drive/Hate Speech/english_dataset.tsv', sep = '\\t', encoding=\"latin-1\")\n",
        "test = pd.read_csv('/content/drive/My Drive/Hate Speech/hasoc2019_en_test-2919.tsv', sep = '\\t', encoding=\"latin-1\")"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOYez7m5w8BH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=train[train.task_3!=\"NONE\"]       #drop none\n",
        "test=test[test.task_3!=\"NONE\"]\n",
        "train=train.reset_index(drop=True)\n",
        "test=test.reset_index(drop=True)\n",
        "X_train = train[\"text\"]\n",
        "y_train = train[\"task_3\"]                     #For task 3         \n",
        "X_test = test[\"text\"]\n",
        "y_test = test[\"task_3\"]"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1dS_VTGBBjm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "cc6ada4b-1434-45cd-8559-04ca19f997a0"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       TIN\n",
              "1       TIN\n",
              "2       TIN\n",
              "3       TIN\n",
              "4       TIN\n",
              "       ... \n",
              "2256    UNT\n",
              "2257    UNT\n",
              "2258    TIN\n",
              "2259    UNT\n",
              "2260    UNT\n",
              "Name: task_3, Length: 2261, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAGwPh34Nn4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "735b3824-962b-43b1-de19-c9902329478f"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NWa1dOIE1bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Remove stopwords from the text\n",
        "stop_words = set(stopwords.words('english')) \n",
        "ix=0\n",
        "for i in X_train:\n",
        "  word_tokens = (i[0].split())  \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        X_train[ix]=X_train[ix]+(w) \n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  word_tokens = (i[0].split())  \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        X_test[ix]= X_test[ix]+(w) \n",
        "  ix=ix+1"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYcf7mNm1gI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove punctuations\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "def clean_text(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        if punct in x:\n",
        "            x = x.replace(punct, '')\n",
        "    return x"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmGlWiP95MuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix=0\n",
        "for i in X_train:\n",
        "  X_train[ix]=clean_text(i)\n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  X_test[ix]=clean_text(i)\n",
        "  ix=ix+1"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wflYqloM5G4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clean the numbers\n",
        "def clean_numbers(x):\n",
        "    if bool(re.search(r'\\d', x)):\n",
        "        x = re.sub('[0-9]{5,}', '#####', x)\n",
        "        x = re.sub('[0-9]{4}', '####', x)\n",
        "        x = re.sub('[0-9]{3}', '###', x)\n",
        "        x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWXG_29D1lvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix=0\n",
        "for i in X_train:\n",
        "  X_train[ix]=clean_numbers(i)\n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  X_test[ix]=clean_numbers(i)\n",
        "  ix=ix+1"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtj562wk6T9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove contractions\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E744km496VH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix=0\n",
        "for i in X_train:\n",
        "  X_train[ix]=replace_contractions(i)\n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  X_test[ix]=replace_contractions(i)\n",
        "  ix=ix+1"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITdafg44_T7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stemming\n",
        "'''Stemming is the process of converting words to their base forms using crude Heuristic rules.\n",
        "   For example, one rule could be to remove ’s’ from the end of any word, so that ‘cats’ becomes ‘cat’. \n",
        "   or another rule could be to replace ‘ies’ with ‘i’ so that ‘ponies becomes ‘poni’.'''\n",
        "from nltk.stem import  SnowballStemmer\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "def stem_text(text):\n",
        "    tokenizer = ToktokTokenizer()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(tokens)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qppnppC_cNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix=0\n",
        "for i in X_train:\n",
        "  X_train[ix]=stem_text(i)\n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  X_test[ix]=stem_text(i)\n",
        "  ix=ix+1"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhvD5-MNw9TX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "421b3d7d-0865-4ff1-cf40-614792519144"
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrspY3inhDNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#After removing or cleaning the text\n",
        "max_features = 11118  #No. of different words in total text\n",
        "maxlen = 70           #Padding length\n",
        "embed_size = 300      #No. of dimensions of glove\n",
        "\n",
        "threshold = 0.35\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBwsgvhHhl5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train.map({'TIN':0, 'UNT': 1})    #Map accordingly \n",
        "y_test = y_test.map({'TIN':0, 'UNT': 1})"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP4di6A42kBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=keras.utils.to_categorical(y_train, num_classes=2)  #one hot encoding\n",
        "y_test=keras.utils.to_categorical(y_test, num_classes=2)\n"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsdmn7n753U4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=[]\n",
        "for i in y_train:\n",
        " y.append(np.argmax(i))"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIwcc-NthnbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wab1NQbl92eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_filters = 42\n",
        "filter_sizes = [1,2,3,5]  #ngrams"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_Lhci79vNoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D,LSTM,SimpleRNN,GRU\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.engine.topology import Layer\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from keras.initializers import *\n",
        "from keras.optimizers import *\n",
        "import keras.backend as K\n",
        "from keras.callbacks import *\n",
        "\n",
        "def model_train_cv(x_train,y_train,nfold,model_obj):\n",
        "    splits = list(StratifiedKFold(n_splits=nfold, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
        "    x_train = x_train\n",
        "    y_train = np.array(y_train)\n",
        "    # matrix for the out-of-fold predictions\n",
        "    train_oof_preds = np.zeros((x_train.shape[0]))\n",
        "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
        "        print(f'Fold {i + 1}')\n",
        "        x_train_fold = x_train[train_idx.astype(int)]\n",
        "        y_train_fold = y_train[train_idx.astype(int)]\n",
        "        x_val_fold = x_train[valid_idx.astype(int)]\n",
        "        y_val_fold = y_train[valid_idx.astype(int)]\n",
        "        # Changed it here a little bit since the custom attention layer is not getting deepcopy\n",
        "        clf = model_obj\n",
        "        clf.load_weights('model.h5')\n",
        "        clf.fit(x_train_fold, y_train_fold, batch_size=512, epochs=5, validation_data=(x_val_fold, y_val_fold))\n",
        "        \n",
        "        valid_preds_fold = clf.predict(x_val_fold)[:,0]\n",
        "\n",
        "        # storing OOF predictions\n",
        "        train_oof_preds[valid_idx] = valid_preds_fold\n",
        "    return train_oof_preds"
      ],
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8ZcLmIb8IdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model of bi-lstm with attention\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "    \n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "\n",
        "def model_lstm_atten(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
        "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
        "    x = AttentionWithContext()(x)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = Dense(2, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAk9X7ByvfND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model of lstm with attention\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "    \n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "\n",
        "def model_slstm_atten(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "    x = AttentionWithContext()(x)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = Dense(2, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq5rsKgyv080",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model of gru with attention\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "    \n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "\n",
        "def model_gru_atten(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
        "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
        "    x = AttentionWithContext()(x)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = Dense(2, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShKIosLhwO-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model of Simple RNN with attention\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "    \n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        uit = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            uit += self.b\n",
        "\n",
        "        uit = K.tanh(uit)\n",
        "        ait = dot_product(uit, self.u)\n",
        "\n",
        "        a = K.exp(ait)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "\n",
        "def model_rnn_atten(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x = Bidirectional(SimpleRNN(128, return_sequences=True))(x)\n",
        "    x = Bidirectional(SimpleRNN(64, return_sequences=True))(x)\n",
        "    x = AttentionWithContext()(x)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = Dense(2, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os7L-_L-1E9C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdbae6b2-3c5d-44cb-ae91-b804b5c306d0"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjdZQ6CivX6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls_model = model_lstm_atten(embedding_matrix) #model of bi-lstm with attention"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2mQmYBpv9pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sls_model = model_slstm_atten(embedding_matrix) #model of lstm with attention"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS775wFCwC7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru_model = model_gru_atten(embedding_matrix) #model of gru with attention"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpGGm6aLwXAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model = model_rnn_atten(embedding_matrix) #model of rnn with attention"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8J_qWpbdihB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95,\n",
        "                                              random_state=42) #split this into train and validation"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aomeeixP8jb5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "310d6dd4-693f-4d42-e86a-f0b7cf2aa276"
      },
      "source": [
        "ls_hist = ls_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                  verbose=2)      #Train bi-lstm with attention"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2147 samples, validate on 114 samples\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4593 - accuracy: 0.9017 - val_loss: 0.3501 - val_accuracy: 0.8947\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3313 - accuracy: 0.9031 - val_loss: 0.3351 - val_accuracy: 0.8947\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3249 - accuracy: 0.9031 - val_loss: 0.3399 - val_accuracy: 0.8947\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3210 - accuracy: 0.9031 - val_loss: 0.3376 - val_accuracy: 0.8947\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3185 - accuracy: 0.9031 - val_loss: 0.3397 - val_accuracy: 0.8947\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3184 - accuracy: 0.9031 - val_loss: 0.3373 - val_accuracy: 0.8947\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3187 - accuracy: 0.9031 - val_loss: 0.3382 - val_accuracy: 0.8947\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3184 - accuracy: 0.9031 - val_loss: 0.3371 - val_accuracy: 0.8947\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3180 - accuracy: 0.9031 - val_loss: 0.3370 - val_accuracy: 0.8947\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3175 - accuracy: 0.9031 - val_loss: 0.3354 - val_accuracy: 0.8947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogHvVYE9wqfj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "e1481de0-e460-421f-f94d-6d4f12ae8d8d"
      },
      "source": [
        "sls_hist = sls_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                  verbose=2)    #Train lstm with attention"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2147 samples, validate on 114 samples\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.4385 - accuracy: 0.8933 - val_loss: 0.3525 - val_accuracy: 0.8947\n",
            "Epoch 2/10\n",
            " - 2s - loss: 0.3291 - accuracy: 0.9031 - val_loss: 0.3362 - val_accuracy: 0.8947\n",
            "Epoch 3/10\n",
            " - 2s - loss: 0.3246 - accuracy: 0.9031 - val_loss: 0.3365 - val_accuracy: 0.8947\n",
            "Epoch 4/10\n",
            " - 2s - loss: 0.3179 - accuracy: 0.9031 - val_loss: 0.3355 - val_accuracy: 0.8947\n",
            "Epoch 5/10\n",
            " - 2s - loss: 0.3180 - accuracy: 0.9031 - val_loss: 0.3349 - val_accuracy: 0.8947\n",
            "Epoch 6/10\n",
            " - 2s - loss: 0.3160 - accuracy: 0.9031 - val_loss: 0.3331 - val_accuracy: 0.8947\n",
            "Epoch 7/10\n",
            " - 2s - loss: 0.3170 - accuracy: 0.9031 - val_loss: 0.3342 - val_accuracy: 0.8947\n",
            "Epoch 8/10\n",
            " - 2s - loss: 0.3150 - accuracy: 0.9031 - val_loss: 0.3290 - val_accuracy: 0.8947\n",
            "Epoch 9/10\n",
            " - 2s - loss: 0.3110 - accuracy: 0.9031 - val_loss: 0.3181 - val_accuracy: 0.8947\n",
            "Epoch 10/10\n",
            " - 2s - loss: 0.2908 - accuracy: 0.9031 - val_loss: 0.2983 - val_accuracy: 0.8947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0vakaVYw0FM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "aeffd3b2-4ad7-4a3d-f2f0-81f560fde95a"
      },
      "source": [
        "gru_hist = gru_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                  verbose=2)      #Train GRU with attention"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2147 samples, validate on 114 samples\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.4445 - accuracy: 0.8812 - val_loss: 0.3531 - val_accuracy: 0.8947\n",
            "Epoch 2/10\n",
            " - 3s - loss: 0.3349 - accuracy: 0.9031 - val_loss: 0.3437 - val_accuracy: 0.8947\n",
            "Epoch 3/10\n",
            " - 3s - loss: 0.3211 - accuracy: 0.9031 - val_loss: 0.3371 - val_accuracy: 0.8947\n",
            "Epoch 4/10\n",
            " - 3s - loss: 0.3131 - accuracy: 0.9031 - val_loss: 0.3340 - val_accuracy: 0.8947\n",
            "Epoch 5/10\n",
            " - 3s - loss: 0.3053 - accuracy: 0.9031 - val_loss: 0.3261 - val_accuracy: 0.8947\n",
            "Epoch 6/10\n",
            " - 3s - loss: 0.2975 - accuracy: 0.9031 - val_loss: 0.3167 - val_accuracy: 0.8947\n",
            "Epoch 7/10\n",
            " - 3s - loss: 0.2850 - accuracy: 0.9031 - val_loss: 0.3105 - val_accuracy: 0.8947\n",
            "Epoch 8/10\n",
            " - 3s - loss: 0.2602 - accuracy: 0.9036 - val_loss: 0.3034 - val_accuracy: 0.8860\n",
            "Epoch 9/10\n",
            " - 3s - loss: 0.2349 - accuracy: 0.9087 - val_loss: 0.3131 - val_accuracy: 0.8947\n",
            "Epoch 10/10\n",
            " - 3s - loss: 0.2114 - accuracy: 0.9148 - val_loss: 0.3339 - val_accuracy: 0.8947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_vYITbcw6op",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "10d213e0-83c8-4ea1-9efa-cde46eed7654"
      },
      "source": [
        "rnn_hist = rnn_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                  verbose=2)      #Train RNN with attention"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2147 samples, validate on 114 samples\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.3952 - accuracy: 0.8975 - val_loss: 0.3471 - val_accuracy: 0.8947\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.3258 - accuracy: 0.9031 - val_loss: 0.3371 - val_accuracy: 0.8947\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.3092 - accuracy: 0.9031 - val_loss: 0.3364 - val_accuracy: 0.8947\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.2851 - accuracy: 0.9041 - val_loss: 0.3307 - val_accuracy: 0.8947\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.2580 - accuracy: 0.9041 - val_loss: 0.3389 - val_accuracy: 0.8947\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.2142 - accuracy: 0.9045 - val_loss: 0.3331 - val_accuracy: 0.9035\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.1630 - accuracy: 0.9092 - val_loss: 0.3710 - val_accuracy: 0.9035\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.1280 - accuracy: 0.9404 - val_loss: 0.3693 - val_accuracy: 0.8509\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0924 - accuracy: 0.9702 - val_loss: 0.4435 - val_accuracy: 0.8509\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0560 - accuracy: 0.9823 - val_loss: 0.5714 - val_accuracy: 0.8684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gKkthjNz8-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pSzYa3_y4dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_t=[]\n",
        "for i in y_test:\n",
        "  y_t.append(np.argmax(i))    #Using argmax to find the index of one hot encoding"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWGsz9rSzxrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_ls_pred = ls_model.predict(x_test)      #predict y_test by bi-lstm with attention\n",
        "y_ls_p=[]\n",
        "for i in y_ls_pred:\n",
        "  y_ls_p.append(np.argmax(i))\n"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHMWJ9719I26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "2d19aa0c-4b1b-4e94-b902-10abc3432d3a"
      },
      "source": [
        "print('Model = Attention Bidirectonal LSTM\\n')  \n",
        "print(classification_report(y_t, y_ls_p))"
      ],
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = Attention Bidirectonal LSTM\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92       245\n",
            "           1       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.85       288\n",
            "   macro avg       0.43      0.50      0.46       288\n",
            "weighted avg       0.72      0.85      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFFok56rxDAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_sls_pred = sls_model.predict(x_test)  #predict y_test by lstm with attention\n",
        "y_sls_p=[]\n",
        "for i in y_sls_pred:\n",
        "  y_sls_p.append(np.argmax(i))"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcFHJ7TExGua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "8b4472f8-a7bd-46cd-f77e-8300cbbecfd4"
      },
      "source": [
        "print('Model = Attention LSTM\\n')\n",
        "print(classification_report(y_t, y_sls_p))"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = Attention LSTM\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92       245\n",
            "           1       0.00      0.00      0.00        43\n",
            "\n",
            "    accuracy                           0.85       288\n",
            "   macro avg       0.43      0.50      0.46       288\n",
            "weighted avg       0.72      0.85      0.78       288\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R4K_8-gvxQSb",
        "colab": {}
      },
      "source": [
        "y_gru_pred = gru_model.predict(x_test)    #predict y_test by GRU with attention\n",
        "y_gru_p=[]\n",
        "for i in y_gru_pred:\n",
        "  y_gru_p.append(np.argmax(i))"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Va13oAXAxQSd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "c7af0515-de63-4e8a-a27a-80e9bd5fbc43"
      },
      "source": [
        "print('Model = Attention GRU\\n')\n",
        "print(classification_report(y_t, y_gru_p))"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = Attention GRU\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.88       245\n",
            "           1       0.13      0.07      0.09        43\n",
            "\n",
            "    accuracy                           0.79       288\n",
            "   macro avg       0.49      0.49      0.49       288\n",
            "weighted avg       0.74      0.79      0.76       288\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DJIkpqBHxbbr",
        "colab": {}
      },
      "source": [
        "y_rnn_pred = rnn_model.predict(x_test)    #predict y_test by RNN with attention\n",
        "y_rnn_p=[]\n",
        "for i in y_rnn_pred:\n",
        "  y_rnn_p.append(np.argmax(i))"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FrDWDDMXxbbv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "3030d309-c68a-400c-d729-1e0b7bbb71f1"
      },
      "source": [
        "print('Model = Attention SimpleRNN\\n')\n",
        "print(classification_report(y_t, y_rnn_p))"
      ],
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = Attention SimpleRNN\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.94      0.89       245\n",
            "           1       0.17      0.07      0.10        43\n",
            "\n",
            "    accuracy                           0.81       288\n",
            "   macro avg       0.51      0.50      0.50       288\n",
            "weighted avg       0.75      0.81      0.77       288\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}