{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Glove_Task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwB8fPoMIED4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2d9c03a-7ac6-4e04-ea0f-fd190075b272"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ6MP-TptKog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49e98151-5269-46f6-f9f6-973e55fc34e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykMyHL-keYKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D,GlobalAveragePooling1D\n",
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')'''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MceJKUmGgWQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_FILE = '/content/drive/My Drive/go/glove.twitter.27B.200d.txt'\n",
        "train = pd.read_csv('/content/drive/My Drive/Hate Speech/english_dataset.tsv', sep = '\\t', encoding=\"latin-1\")\n",
        "test = pd.read_csv('/content/drive/My Drive/Hate Speech/hasoc2019_en_test-2919.tsv', sep = '\\t', encoding=\"latin-1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOYez7m5w8BH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train[\"text\"]\n",
        "y_train = train[\"task_1\"]\n",
        "X_test = test[\"text\"]\n",
        "y_test = test[\"task_1\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAGwPh34Nn4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "29795cb3-af02-427f-b2f0-af2570ec2ebd"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NWa1dOIE1bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "ix=0\n",
        "for i in X_train:\n",
        "  word_tokens = (i[0].split())  \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        X_train[ix]=X_train[ix]+(w) \n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  word_tokens = (i[0].split())  \n",
        "  for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        X_test[ix]= X_test[ix]+(w) \n",
        "  ix=ix+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYcf7mNm1gI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove punctuations\n",
        "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "def clean_text(x):\n",
        "    x = str(x)\n",
        "    for punct in puncts:\n",
        "        if punct in x:\n",
        "            x = x.replace(punct, '')\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmGlWiP95MuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix=0\n",
        "for i in X_train:\n",
        "  X_train[ix]=clean_text(i)\n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  X_test[ix]=clean_text(i)\n",
        "  ix=ix+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wflYqloM5G4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clean the numbers\n",
        "def clean_numbers(x):\n",
        "    if bool(re.search(r'\\d', x)):\n",
        "        x = re.sub('[0-9]{5,}', '#####', x)\n",
        "        x = re.sub('[0-9]{4}', '####', x)\n",
        "        x = re.sub('[0-9]{3}', '###', x)\n",
        "        x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWXG_29D1lvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix=0\n",
        "for i in X_train:\n",
        "  X_train[ix]=clean_numbers(i)\n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  X_test[ix]=clean_numbers(i)\n",
        "  ix=ix+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtj562wk6T9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove contractions\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "def _get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re\n",
        "\n",
        "contractions, contractions_re = _get_contractions(contraction_dict)\n",
        "\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E744km496VH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix=0\n",
        "for i in X_train:\n",
        "  X_train[ix]=replace_contractions(i)\n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  X_test[ix]=replace_contractions(i)\n",
        "  ix=ix+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITdafg44_T7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import  SnowballStemmer\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "def stem_text(text):\n",
        "    tokenizer = ToktokTokenizer()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qppnppC_cNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ix=0\n",
        "for i in X_train:\n",
        "  X_train[ix]=stem_text(i)\n",
        "  ix=ix+1\n",
        "ix=0\n",
        "for i in X_test:\n",
        "  X_test[ix]=stem_text(i)\n",
        "  ix=ix+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhvD5-MNw9TX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "461d93a5-b4a0-408c-892b-b7a24d13e25b"
      },
      "source": [
        "type(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrspY3inhDNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "max_features = 22107\n",
        "maxlen = 70\n",
        "embed_size = 200\n",
        "\n",
        "threshold = 0.35\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBwsgvhHhl5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train.map({'NOT':0, 'HOF': 1})\n",
        "y_test = y_test.map({'NOT':0, 'HOF': 1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP4di6A42kBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=keras.utils.to_categorical(y_train, num_classes=2)\n",
        "y_test=keras.utils.to_categorical(y_test, num_classes=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsdmn7n753U4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=[]\n",
        "for i in y_train:\n",
        " y.append(np.argmax(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JZSMVvD6Ejd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ef7e64a-d7d6-42bc-9744-4671fffeecee"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIwcc-NthnbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42EhNCOQwCU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_filters = 42\n",
        "filter_sizes = [1,2,3,5]\n",
        "\n",
        "def get_model():    \n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "#    x = SpatialDropout1D(0.4)(x)\n",
        "    x = Reshape((maxlen, embed_size, 1))(x)\n",
        "    \n",
        "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size),\n",
        "                                 kernel_initializer='he_normal', activation='tanh')(x)\n",
        "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size),\n",
        "                                 kernel_initializer='he_normal', activation='tanh')(x)\n",
        "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), \n",
        "                                 kernel_initializer='he_normal', activation='tanh')(x)\n",
        "    conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embed_size),\n",
        "                                 kernel_initializer='he_normal', activation='tanh')(x)\n",
        "    \n",
        "    maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1))(conv_0)\n",
        "    maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1))(conv_1)\n",
        "    maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1))(conv_2)\n",
        "    maxpool_3 = MaxPool2D(pool_size=(maxlen - filter_sizes[3] + 1, 1))(conv_3)\n",
        "        \n",
        "    z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
        "    z = Flatten()(z)\n",
        "    z = Dropout(0.1)(z)\n",
        "        \n",
        "    outp = Dense(2, activation=\"sigmoid\")(z)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHgaJcsnPx_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN\n",
        "from keras.layers import Dense, Input, SimpleRNN, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D,concatenate\n",
        "def model_srr_du(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    '''\n",
        "    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?\n",
        "        64*70(maxlen)*2(bidirection concat)\n",
        "    SimpleRNN is fast implementation of LSTM layer in Keras which only runs on GPU\n",
        "    '''\n",
        "    x = Bidirectional(SimpleRNN(64, return_sequences=True))(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(64, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(2, activation=\"sigmoid\")(conc)\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "srr_model=model_srr_du(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chY81tn_RMM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D,concatenate\n",
        "def model_lstm_du(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    '''\n",
        "    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?\n",
        "        64*70(maxlen)*2(bidirection concat)\n",
        "    LSTM is fast implementation of LSTM layer in Keras which only runs on GPU\n",
        "    '''\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(64, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(2, activation=\"sigmoid\")(conc)\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "lstm_model=model_lstm_du(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8ZcLmIb8IdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BiDirectional LSTM\n",
        "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D,concatenate\n",
        "def model_lstm_du(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    '''\n",
        "    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?\n",
        "        64*70(maxlen)*2(bidirection concat)\n",
        "    CuDNNLSTM is fast implementation of LSTM layer in Keras which only runs on GPU\n",
        "    '''\n",
        "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(64, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(2, activation=\"sigmoid\")(conc)\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "ls_model=model_lstm_du(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCCJdLNDO6jH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRU\n",
        "from keras.layers import Dense, Input, GRU, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D,concatenate\n",
        "def model_gru_du(embedding_matrix):\n",
        "    inp = Input(shape=(maxlen,))\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    '''\n",
        "    Here 64 is the size(dim) of the hidden state vector as well as the output vector. Keeping return_sequence we want the output for the entire sequence. So what is the dimension of output for this layer?\n",
        "        64*70(maxlen)*2(bidirection concat)\n",
        "    GRU is fast implementation of LSTM layer in Keras which only runs on GPU\n",
        "    '''\n",
        "    x = Bidirectional(GRU(64, return_sequences=True))(x)\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    conc = concatenate([avg_pool, max_pool])\n",
        "    conc = Dense(64, activation=\"relu\")(conc)\n",
        "    conc = Dropout(0.1)(conc)\n",
        "    outp = Dense(2, activation=\"sigmoid\")(conc)\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "gr_model=model_gru_du(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-fi7MXkyxml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "f08d7d8d-b032-46e3-d7ab-afb860e92bcb"
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "\n",
        "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95,\n",
        "                                              random_state=233)\n",
        "\n",
        "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                  verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5559 samples, validate on 293 samples\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.6678 - accuracy: 0.6041 - val_loss: 0.6613 - val_accuracy: 0.6382\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.6183 - accuracy: 0.6463 - val_loss: 0.6509 - val_accuracy: 0.6621\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.5511 - accuracy: 0.7212 - val_loss: 0.6426 - val_accuracy: 0.6553\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.4488 - accuracy: 0.8059 - val_loss: 0.6386 - val_accuracy: 0.6621\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.3510 - accuracy: 0.8743 - val_loss: 0.6541 - val_accuracy: 0.6519\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.2581 - accuracy: 0.9340 - val_loss: 0.6769 - val_accuracy: 0.6758\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.1813 - accuracy: 0.9658 - val_loss: 0.6993 - val_accuracy: 0.6621\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.1242 - accuracy: 0.9835 - val_loss: 0.7283 - val_accuracy: 0.6587\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0855 - accuracy: 0.9912 - val_loss: 0.7668 - val_accuracy: 0.6485\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0627 - accuracy: 0.9923 - val_loss: 0.8072 - val_accuracy: 0.6519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPQDV9eNQVn0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "7b2667b8-5784-4666-e486-a256f2c37d40"
      },
      "source": [
        "srr_hist = srr_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                  verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5559 samples, validate on 293 samples\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.6792 - accuracy: 0.6001 - val_loss: 0.6650 - val_accuracy: 0.6246\n",
            "Epoch 2/10\n",
            " - 3s - loss: 0.6534 - accuracy: 0.6132 - val_loss: 0.6551 - val_accuracy: 0.6246\n",
            "Epoch 3/10\n",
            " - 3s - loss: 0.6412 - accuracy: 0.6206 - val_loss: 0.6566 - val_accuracy: 0.6246\n",
            "Epoch 4/10\n",
            " - 3s - loss: 0.5864 - accuracy: 0.6868 - val_loss: 0.6497 - val_accuracy: 0.6621\n",
            "Epoch 5/10\n",
            " - 3s - loss: 0.4738 - accuracy: 0.7816 - val_loss: 0.6994 - val_accuracy: 0.5939\n",
            "Epoch 6/10\n",
            " - 3s - loss: 0.3236 - accuracy: 0.8813 - val_loss: 0.7897 - val_accuracy: 0.5461\n",
            "Epoch 7/10\n",
            " - 3s - loss: 0.1969 - accuracy: 0.9432 - val_loss: 0.8285 - val_accuracy: 0.6314\n",
            "Epoch 8/10\n",
            " - 3s - loss: 0.1004 - accuracy: 0.9763 - val_loss: 0.9164 - val_accuracy: 0.6382\n",
            "Epoch 9/10\n",
            " - 3s - loss: 0.0550 - accuracy: 0.9903 - val_loss: 0.9981 - val_accuracy: 0.6416\n",
            "Epoch 10/10\n",
            " - 3s - loss: 0.0378 - accuracy: 0.9932 - val_loss: 1.0652 - val_accuracy: 0.6109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfXHOOxvRgOD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "92ec4e9a-e0b9-400a-fbf0-295fd1d82952"
      },
      "source": [
        "lstm_hist = lstm_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                  verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5559 samples, validate on 293 samples\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.6605 - accuracy: 0.6154 - val_loss: 0.6453 - val_accuracy: 0.6246\n",
            "Epoch 2/10\n",
            " - 4s - loss: 0.6198 - accuracy: 0.6573 - val_loss: 0.6239 - val_accuracy: 0.6655\n",
            "Epoch 3/10\n",
            " - 4s - loss: 0.5609 - accuracy: 0.7163 - val_loss: 0.6081 - val_accuracy: 0.6655\n",
            "Epoch 4/10\n",
            " - 4s - loss: 0.4797 - accuracy: 0.7692 - val_loss: 0.6389 - val_accuracy: 0.6826\n",
            "Epoch 5/10\n",
            " - 4s - loss: 0.3482 - accuracy: 0.8507 - val_loss: 0.7684 - val_accuracy: 0.6758\n",
            "Epoch 6/10\n",
            " - 4s - loss: 0.2156 - accuracy: 0.9199 - val_loss: 0.8422 - val_accuracy: 0.6382\n",
            "Epoch 7/10\n",
            " - 4s - loss: 0.1210 - accuracy: 0.9611 - val_loss: 1.1032 - val_accuracy: 0.6416\n",
            "Epoch 8/10\n",
            " - 4s - loss: 0.0685 - accuracy: 0.9793 - val_loss: 1.2846 - val_accuracy: 0.5870\n",
            "Epoch 9/10\n",
            " - 4s - loss: 0.0485 - accuracy: 0.9845 - val_loss: 1.6073 - val_accuracy: 0.6485\n",
            "Epoch 10/10\n",
            " - 4s - loss: 0.0287 - accuracy: 0.9924 - val_loss: 1.6743 - val_accuracy: 0.6382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aomeeixP8jb5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "f36a6286-5729-470f-db9b-3bb9f91f0b05"
      },
      "source": [
        "ls_hist = ls_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                  verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5559 samples, validate on 293 samples\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.6614 - accuracy: 0.6122 - val_loss: 0.6500 - val_accuracy: 0.6212\n",
            "Epoch 2/10\n",
            " - 1s - loss: 0.6237 - accuracy: 0.6528 - val_loss: 0.6303 - val_accuracy: 0.6314\n",
            "Epoch 3/10\n",
            " - 1s - loss: 0.5664 - accuracy: 0.7095 - val_loss: 0.6440 - val_accuracy: 0.6109\n",
            "Epoch 4/10\n",
            " - 1s - loss: 0.4771 - accuracy: 0.7701 - val_loss: 0.6616 - val_accuracy: 0.6997\n",
            "Epoch 5/10\n",
            " - 1s - loss: 0.3641 - accuracy: 0.8403 - val_loss: 0.7888 - val_accuracy: 0.6075\n",
            "Epoch 6/10\n",
            " - 1s - loss: 0.2194 - accuracy: 0.9167 - val_loss: 0.8517 - val_accuracy: 0.6075\n",
            "Epoch 7/10\n",
            " - 1s - loss: 0.1166 - accuracy: 0.9622 - val_loss: 1.1603 - val_accuracy: 0.6177\n",
            "Epoch 8/10\n",
            " - 1s - loss: 0.0661 - accuracy: 0.9788 - val_loss: 1.5797 - val_accuracy: 0.5802\n",
            "Epoch 9/10\n",
            " - 1s - loss: 0.0415 - accuracy: 0.9874 - val_loss: 1.5037 - val_accuracy: 0.5973\n",
            "Epoch 10/10\n",
            " - 1s - loss: 0.0296 - accuracy: 0.9930 - val_loss: 1.8612 - val_accuracy: 0.6007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfK3V6WnPIzM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "0181bef0-4d56-47aa-876c-38e621c9facb"
      },
      "source": [
        "gr_hist = gr_model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs,\n",
        "                 validation_data=(X_val, y_val),\n",
        "                  verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5559 samples, validate on 293 samples\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.6629 - accuracy: 0.6113 - val_loss: 0.6557 - val_accuracy: 0.6246\n",
            "Epoch 2/10\n",
            " - 5s - loss: 0.6282 - accuracy: 0.6427 - val_loss: 0.6434 - val_accuracy: 0.6416\n",
            "Epoch 3/10\n",
            " - 5s - loss: 0.5731 - accuracy: 0.7023 - val_loss: 0.6260 - val_accuracy: 0.6655\n",
            "Epoch 4/10\n",
            " - 5s - loss: 0.4909 - accuracy: 0.7600 - val_loss: 0.6372 - val_accuracy: 0.6826\n",
            "Epoch 5/10\n",
            " - 5s - loss: 0.3779 - accuracy: 0.8412 - val_loss: 0.7113 - val_accuracy: 0.6314\n",
            "Epoch 6/10\n",
            " - 5s - loss: 0.2383 - accuracy: 0.9149 - val_loss: 0.8416 - val_accuracy: 0.5973\n",
            "Epoch 7/10\n",
            " - 5s - loss: 0.1222 - accuracy: 0.9664 - val_loss: 0.9900 - val_accuracy: 0.6143\n",
            "Epoch 8/10\n",
            " - 5s - loss: 0.0561 - accuracy: 0.9872 - val_loss: 1.1427 - val_accuracy: 0.6041\n",
            "Epoch 9/10\n",
            " - 5s - loss: 0.0335 - accuracy: 0.9930 - val_loss: 1.3045 - val_accuracy: 0.6007\n",
            "Epoch 10/10\n",
            " - 5s - loss: 0.0240 - accuracy: 0.9950 - val_loss: 1.3977 - val_accuracy: 0.5973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gKkthjNz8-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pSzYa3_y4dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_test)\n",
        "y_p=[]\n",
        "y_t=[]\n",
        "for i in y_pred:\n",
        "  y_p.append(np.argmax(i))\n",
        "for i in y_test:\n",
        "  y_t.append(np.argmax(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdhd5T_dzmGD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "b48241a5-232f-49ad-cf3e-b65bb432c5c5"
      },
      "source": [
        "print('Model = CNN\\n')\n",
        "print(classification_report(y_t, y_p))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = CNN\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.76      0.82       865\n",
            "           1       0.49      0.69      0.57       288\n",
            "\n",
            "    accuracy                           0.74      1153\n",
            "   macro avg       0.69      0.73      0.70      1153\n",
            "weighted avg       0.78      0.74      0.76      1153\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQRxeGbaQEAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_srr_pred = srr_model.predict(x_test)\n",
        "y_srr_p=[]\n",
        "for i in y_srr_pred:\n",
        "  y_srr_p.append(np.argmax(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqRjosc1Qf0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "e80c6e61-97a9-48be-b3e8-14f85a7f5321"
      },
      "source": [
        "print('Model = SimpleRNN\\n')\n",
        "print(classification_report(y_t, y_srr_p))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = SimpleRNN\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.68      0.76       865\n",
            "           1       0.40      0.63      0.49       288\n",
            "\n",
            "    accuracy                           0.67      1153\n",
            "   macro avg       0.62      0.66      0.62      1153\n",
            "weighted avg       0.74      0.67      0.69      1153\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tceoivL8Rqle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_lstm_pred = lstm_model.predict(x_test)\n",
        "y_lstm_p=[]\n",
        "for i in y_lstm_pred:\n",
        "  y_lstm_p.append(np.argmax(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJyXh2hyRyqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "e55f3eab-67d3-400b-dc60-a1f2b6a39ca8"
      },
      "source": [
        "print('Model = LSTM\\n')\n",
        "print(classification_report(y_t, y_lstm_p))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = LSTM\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.77      0.80       865\n",
            "           1       0.45      0.58      0.51       288\n",
            "\n",
            "    accuracy                           0.72      1153\n",
            "   macro avg       0.65      0.67      0.66      1153\n",
            "weighted avg       0.75      0.72      0.73      1153\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWGsz9rSzxrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_ls_pred = ls_model.predict(x_test)\n",
        "y_ls_p=[]\n",
        "for i in y_ls_pred:\n",
        "  y_ls_p.append(np.argmax(i))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHMWJ9719I26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "9d03e14a-9776-4cd7-d4a5-8c426899edbb"
      },
      "source": [
        "print('Model = Bidirectonal LSTM\\n')\n",
        "print(classification_report(y_t, y_ls_p))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = Bidirectonal LSTM\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.69      0.76       865\n",
            "           1       0.39      0.61      0.48       288\n",
            "\n",
            "    accuracy                           0.67      1153\n",
            "   macro avg       0.62      0.65      0.62      1153\n",
            "weighted avg       0.73      0.67      0.69      1153\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w78mbVGPT2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_gr_pred = gr_model.predict(x_test)\n",
        "y_gr_p=[]\n",
        "for i in y_gr_pred:\n",
        "  y_gr_p.append(np.argmax(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNvsZNPQPbu8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "aae29c4f-07b5-4474-a196-650145c228c0"
      },
      "source": [
        "print('Model = GRU\\n')\n",
        "print(classification_report(y_t, y_gr_p))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model = GRU\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.73      0.79       865\n",
            "           1       0.45      0.67      0.54       288\n",
            "\n",
            "    accuracy                           0.71      1153\n",
            "   macro avg       0.66      0.70      0.67      1153\n",
            "weighted avg       0.76      0.71      0.73      1153\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}